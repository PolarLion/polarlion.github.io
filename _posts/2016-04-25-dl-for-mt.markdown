---
layout: post
type: "tech"
title:  "神经网络机器翻译（Neural Machine Translation）入门不完全指南"
location: 东北老家
date:   2016-04-29 15:00:00 +0800
categories: "NMT"
tags: "神经网络机器翻译 NMT"
---

2003年,神经网络概率语言模型NPLM <a href="#bengio2003">[Bengio 2003]</a>  被提出，神经网络模型开始正式踏足自然语言处理的重要任务。不过由于各种原因，让神经网络在自然语言处理中还是门可罗雀了几年。但后来的故事大家都知道了：日后大放异彩的RNNLM <a href="#mikolov2010">[Mikolov et al 2010]</a> ，风光无限噱头满满的word2vec <a href="#mikolov2013">[Mikolov et al 2013]</a> ，人们已经不再满足于“神经网络”，“深度学习（Deep Learning）”才显得更加时髦。本文试图给有意了解神经网络机器翻译的初学者整理并列举了些重要文献，节省求学路上的米娜查找文献的时间，在文献层面帮助大家快速入门。

**以下内容纯属博主一家之言，有错误欢迎指出！**<br />

博主在2915年初入门深度学习在机器翻译上的应用时，参考了2014年底 Kevin Duh 的一份幻灯片 <a href="#duh2014">[Duh 2014]</a> <a target="_blank" href="http://www.cs.jhu.edu/~kevinduh/notes/cwmt14tutorial.pdf">http://www.cs.jhu.edu/~kevinduh/notes/cwmt14tutorial.pdf</a>, 这份slides总结了截至2014年底，深度学习在机器翻译上的一些重要进展，引文完备分类合。对当时来说，幻灯片结合引文足以了解深度学习在机器翻译领域的情况。不幸的是，仅仅过了不到一年，机器翻译领域就被更加猛烈的革命搅得目全非了，非到什么程度最后面会谈。

<h2>神经网络概率语言模型（NPLM）</h2>
重要的事情说三遍不够，56种语言汇成一句话，这篇论文必须看，必须弄懂！只要是有意学自然语言处理的就没法回避这篇论文：神经网络概率语言模型 NPLM <a href="#bengio2003">[Bengio 2003]</a> 。
<br />博主也是最近发现一些孩子每天讨论和使用词向量的时候，竟然不知道它们的本源和最初的动机！知彼知己百战不殆，统计自然语言处理多少还是要了解的，所以在这里特别地0公式介绍一下。<br />
传统的语言模型，比如最最最经典的N元语法模型（n-gram）是基于计数统计的。翻译成白话文就是：先对训练集合统计词频，然后利用概率论的方法计算概率。它至今仍旧快速有效，但也有致命伤（不然就轮不到我们的 NPLM 出场了）。
<br />举个栗子，“**<font color="#3104B4">鼩鼱</font>**”这个词，不看《疯狂动物城》应该没几个人认识（没错《疯狂动物城》里面的 Mr Big 就是鼩鼱）。因为N元语法模型基于概率连乘，那么含有“鼩鼱”这种词出现的任何词串都无法获得高概率，因为“鼩鼱”的统计计数太少了，这样正确无误的话也会得到较低的概率分。
NPLM 就不同了，它的学习不涉及计数，只根据上下文，并且利用模型自身特点为每个词增加了一个向量表示层，这样相似的上下文容易学习得到相似的词向量进而得到相似的输出。
<br />举个栗子，假设训练集合里面含有下面两句话：
<table>
	<tr><td bgcolor="#E6F7F4">1. “<font color="#04B431">仓鼠</font>是一种体型很小的哺乳动物。”（在训练集出现100次）</td></tr>
	<tr><td bgcolor="#E6F7F4">2. “<font color="#3104B4">鼩鼱</font>是一种体型很小的哺乳动物。”（在训练集出现2次）</td></tr>
</table>
<br />
上面栗子中“**<font color="#04B431">仓鼠</font>**”和“**<font color="#3104B4">鼩鼱</font>**”的上下文完全一致，所以“**<font color="#3104B4">鼩鼱</font>**”这个词会获得同“**<font color="#04B431">仓鼠</font>**”相似的词向量，进而日后计算概率的时候两词的运算结果相似。这就是神经网络概率语言模型 NPLM 的动机。总结起来，NPLM 首先是一个语言模型，其次它可以学习得到词的向量化表示，并且正是靠这种向量化表示解决了**低频词**的问题。后来的各种词向量化方法的源头大都离不开NPLM的思想。


<h2>餐前小菜</h2>
<h3>两条时间线</h3>
2003年到2011年间，各种升级版的 NPLM 层出不穷却也数量不多。2010年，Mikolov 发表了RNNLM <a href="#mikolov2010">[Mikolov et al 2010]</a> 的论文。这样，语言模型方面，不但有了前向神经网络老大哥 NPLM <a href="#bengio2003">[Bengio 2003]</a> ，还搭配了拥有无限可能却有点儿“喜新厌旧”的再现神经网络语言模型（Recurrent Neural Network Language Model，RNNLM） <a href="#mikolov2010">[Mikolov et al 2010]</a> 。此时的硬件技术也和十年前不可同日而语。看上去深度学习的刀已磨的差不多，只等着闯入自然语言处理的江湖大杀四方了。

另一方面，那时的统计机器翻译（Statical Machine Translation，以下简称 SMT）正走向成熟。2002年ACL（Association for Computational Linguistics）年会上的最佳论文 <a href="#och2002">[Och 2002]</a> 为 SMT 引入最大熵模型，这样的模型更加容易添加新的特征，扩展性空前的好，从此将统计机器翻译带入黄金时期，各种各样的模型和特征函数被引入其中，翻译效果逐渐被人接受。开源的机器翻译软件 moses <a href="#koehn2007">[Koehn 2007]</a> 公布后，从工具层面降低了 SMT 的门槛，SMT 变得更加普及。可那时的学者应该想象不到：仅仅培养一拨本科生的工夫，机器翻译界就天翻地覆了。

<h3>甘当绿叶</h3>
<h4>--深度学习方法作为特征函数出现在统计机器翻译中</h4>
上面提到，<a href="#och2002">[Och 2002]</a> 的工作让统计机器翻译很方便的引入各种新的特征函数，而且时间已经到了2012年，再不做点儿什么可能会迎来世界末日，于是 <a href="#schwenk2012">[Schwenk 2012]</a> 引入神经网络作为统计机器翻译中的翻译模型，以提升 SMT 的效果。这部分故事在 <a href="#duh2014">[Duh 2014]</a> 的幻灯片中描述的很细致，博主不想展开去谈，聪明的读者去看 Duh 的幻灯片就好。但博主却不得不介绍 <a href="#devlin2014">[Devlin et al. 2014]</a> 这篇2014年的ACL年会最佳论文。
<a href="#devlin2014">[Devlin et al. 2014]</a>这篇论文用的都是老套路：用NPLM <a href="#bengio2003">[Bengio 2003]</a> 作为模型主体，用联合模型（Joint Model）的方法同时对源语言（将要被翻译的句子）和目标语言（翻译生成的句子）进行建模。作者还结合自己提出的和当时成熟的优化算法提升模型的解码和训练速度，又通过大量的实验验证模型的有效性。笔者看来，这篇文章赢在扎实上，各个部分都谈不上突破性的创新，但是文章的每个细节都顾及到，给人以安全感，让人敢花时间去重现这个工作。没过多久开源工具 moses <a href="#koehn2007">[Koehn 2007]</a> 就集成了<a href="#devlin2014">[Devlin et al. 2014]</a>这个工作，博主亲自在汉-英，英-法翻译中实验，在基于短语（phrase-based）的翻译模型中，确实对翻译效果有稳定的提升！！

<h3>单飞</h3>
<h4>--深度学习方法用作端到端翻译</h4>
2014年，从2003年的 NPLM 算起，已经有11年了。十年磨一剑，也曾辅佐 SMT 驰骋沙场、扩大战果，但是这一天还是来了，神经网络终究要离开 SMT，寻找自己的理想乡。
端到端（end-to-end）翻译，其实是针对老搭档SMT的管道（pipline）模式说的，一图胜千言，我们来看看 SMT 的训练过程：
<br />
<br />
<img  src="{{ site.baseurl }}/images/smtpipline.jpg" align="middle" height="400px" width="500px" />
<br />
<br />
看上去好像并不复杂对不对，但是不得不说的是，现在统计机器翻译利用的主要模型就有十几种，例如 moses 的 baseline 系统，光翻译模型就有四种，使用的模型总数有15个（只是 baseline 哦！），有些模型彼此独立有些相互关联，不同的模型靠对数线性模型组合在一起，再进行调优训练，模型的调优采用的是 <a href="#och2003">[Och 2003]</a> 的方法，参数越多，调优的复杂度越高，也越容易陷入局部最优解。
<br />
何为端到端呢？再给一张图：
<br />
<br />
<img  src="{{ site.baseurl }}/images/end2end.jpg" align="middle" height="180px" width="300px" />
<br />
<br />
我也觉得这两张图有点儿土，但是帮助领会精神还是有效的，端到端的翻译模型中并不包含独立的成分，模型是一个整体，简单、自然、专注！如果这样一种简单的模型有效，为什么还要坚持 SMT 的组合模型的路线呢？所以在机器翻译领域神经网络模型一旦单飞成功，必然势不可挡，而事实也**正是如此！**
<br />


<h2>正餐</h2>
<h3>第一道主菜</h3>
在神经网络模型作为SMT的特征函数的时候，就已经有作为翻译模型提供短语翻译的应用，所以“具备翻译功能”的神经网络并不是我们的重点，我们的重点是只用神经网络进行翻译的，符合端到端（end-to-end）的神经网络机器翻译。这个新故事是由 <a href="#sutskever2014">[Sutskever, Vinyals and Le 2014]</a> 拉开序幕的，这是NMT大餐中的第一道主菜。作者的核心思想是实现序列到序列（sequence to sequence）的学习过程，这时候前人的工作其实很多，但 <a href="#sutskever2014">[Sutskever, Vinyals and Le 2014]</a> 选择了 <a href="#cho2014">[Cho et al. 2014]</a> 提出的 RNN 编码器-解码器（Encoder-Decoder） 模型，并采用 LSTM（Long Short Term Memory）<a href="#graves1997">[Graves 1997]</a> 作为RNN的单元。由于采用了 LSTM，使得 <a href="#cho2014">[Cho et al. 2014]</a> RNN Encoder-Decoder 可以学习到更长时间序列的信息，使其从最初的翻译短语升级为有能力翻译完整句子。<a href="#sutskever2014">[Sutskever, Vinyals and Le 2014]</a> 的工作达到了 SMT 的 baseline 水平。这时候是否超过或达到并不重要，人们更在意的是能不能而不是好不好，毕竟这只是开始的开始！
<h3>第二道主菜</h3>
第二道菜上的有些快，但却情有可原，因为作者主要来自Bengio的团队，可以更方便的使用和修改前辈的工作。 <a href="#bahdanau2014">[Bahdanau, Cho and Bengio 2014]</a> 在 <a href="#cho2014">[Cho et al. 2014]</a> 的基础上，增加了“对齐”函数，使得 Encoder-Decoder 框架在翻译长句时更加稳定。这篇论文还开放出了 python 语言编写的源代码 <a href="https://github.com/lisa-groundhog/GroundHog" target="_blank">https://github.com/lisa-groundhog/GroundHog</a>，使用简单.笔者亲测，在汉-英和英-法翻译上，可以达到报告的效果。这篇论文以美观的实验结果，开源的代码和完备的实验数据，不但再次回答了“能不能”问题，甚至向世人展示了“不但可以，还很好用，以后会更好”的信息，NMT从此真的演变成一发不可收拾的革命，大有摧枯拉朽之势。
<h3>无限可能</h3>
<h4>人人都看得到他的缺点，这正是动力所在</h4>
<a href="#sutskever2014">[Sutskever, Vinyals and Le 2014]</a> 发表时，大家都被端到端的美吸引，没人在乎它到底好或者差到什么程度，毕竟没人指望一个婴儿可以背出千位的圆周率，而 <a href="#bahdanau2014">[Bahdanau, Cho and Bengio 2014]</a> 的成果迅速让人们回归对科学问题的理性中——神经网络长久以来面临的问题，在机器翻译这个应用中被放的更大了。反之，如果一一攻克这些问题，眼前这个初世代模型已经可以和 SMT 叫板，再发展下去，恐怕前途无量。下面会介绍，NMT 已经在跨语言交叉翻译，字母解码等 SMT 想都不敢想的应用中取得了进展。<br />
<h4>困难重重？</h4>
笔者凭一己之见总结了NMT面临的一些问题:<br />
1.10,000,000(千万)量级的参数
<ul>
<li>需要更多的语料训练防止过拟合</li>
<li>受限制的词表</li>
<li>漫长的训练时间</li>
<li>严苛的硬件需求</li>
</ul>
2.随时间消失的信息
<ul><li>RNN至今没有完全克服的问题</li></ul>
<br />
<h4>解决方法？</h4>
1.10,000,000(千万)量级的参数
<ul>
<li>需要更多的语料训练防止过拟合
<ul>
	<li>利用大量的单语语料独立训练语言模型，并结合进NMT，这个论文的动机和做法都是比较容易想到的，但是笔者认为论文中的实验部分略有不足，在汉-英实验中明显语料过小（不到一百万平行句对），很不合理：<a href="#gulcehre2015">[Gulcehre et al. 2015]</a></li>
	<li>共享模型参数，利用大语料补充小语料。为作者的目标举个栗子：假设有汉英，英日两种平行语料，通过这个模型，就可以做到汉语和日语的互译！但遗憾的是虽然作者提出可以达到此效果，却没有进行这样的实验（大概是因为没有标注的测试集），不过这部分研究应该是一个不错的研究方向：如何利用已有数据补充不完备的数据：<a href="#firat2016">[Firat, Cho and Bengio 2016]</a></li>
</ul>
</li>
<li>受限制的词表</li>
<ul>
	<li>改进优化目标使其具备在更大词表上运算的能力：<a href="#jean2015">[Jean et al. 2015]</a></li>
	<li>后处理替换未登录词：<a href="#luong2015a">[Luong et al. 2015a]</a></li>
	<li>在目标语言端抛弃词，直接对字母建模。作者一直强调自己只是回答了一个“能不能在字母上做翻译”的问题，并不是突破进展或是提升效果，当然最后答案是肯定的，不过限制了测试用的句长，只能说在一定的限度内对那个问题做了肯定回答。对字母/字符建模应该是一个大家持续感兴趣的话题，虽然工作上很难去follow并取得进展：<a href="#chung2016">[Chung, Cho and Bengio 2016]</a></li>
</ul>
<li>漫长的训练时间</li>
<ul>
	<li>改进的优化算法，“利用特别的优化算法提升训练速度并不损失训练效果”：<a href="#vaswani2013">[Vaswani et al. 2013]</a> <a href="#devlin2014">[Devlin et al. 2014]</a></li>
</ul>
<li>严苛的硬件需求</li>
<ul>
	<li>显卡，显卡，显卡越多越好！！！有集群更好！！！显卡我推荐 Titan X<br />做深度学习而不用显卡是不理智的，一般情况的运算GPU会比CPU速度高出至少一个数量级，比如常规的翻译模型训练在单GPU上需要5天时间，那么换成CPU就需要两个月，不要问我换成CPU集群会怎样。GPU集群会有效，但是普通高校应该没有这种资源</li>
</ul>
</ul>
2.随时间消失的信息
<ul>
	<li>这个问题的一个解法是引入更复杂的RNN结构</li>
	<ul>
		<li>GRU <a href="#cho2014">[Cho et al. 2014]</a> 和 LSTM <a href="#graves1997">[Graves 1997]</a> 算是一个公共的参考答案，目前出现的文章大都以这二者之一作为RNN单元</li>
	</ul>
	<li>光引入复杂的RNN单元是远远不够打败 SMT 的，还要从整体网络结构入手！核心思想就是让网络可以更容易的学到更多东西，所谓的更多包括更长的历史信息和更复杂的语言层面的信息，这部分内容是我们二道主菜领衔！</li>
	<ul>
	<li>无需多言，上面的二道主菜： <a href="#bahdanau2014">[Bahdanau, Cho and Bengio 2014]</a> </li>
	<li>注意力模型（attention model）：<a href="#bahdanau2015b">[Luong et al., 2015]</a> </li>
	<li>清华大学，大男神刘洋老师的团队出品，双向的注意力模型：<a href="#cheng2015">[Cheng et al., 2015]</a> </li>
	<li>想办法把调序和繁衍度模型引入NMT中：<a href="#feng2016">[Feng et al. 2016]</a>，<a href="#tu2016">[Tu et al. 2016]</a> </li>
	</ul>
	<li>接下来这个工作又来自刘洋老师的团队，灵感来自 SMT 中经典的调优算法MERT <a href="#och2003">[Och, 2003]</a> 和 <a href="#gao2014">[Gao et al. 2014]</a> 等，将机器翻译的评价指标引入优化目标参与模型训练，结果是使得模型在长句上表现更好，BLEU值提升最高到7个百分点。笔者前不久与刘洋老师交流时刘洋老师也特别提到这个工作的 “出乎意料” 性，并且指出后期通过更替不同的评价指标 （BLEU，TER等），发现均会得到或多或少的提升！：<a href="#shen2015">[Shen et al. 2015]</a> </li>
</ul>
<br />
<h2>后记</h2>
<h3>面目全非的非</h3>
文中提到 NMT 让机器翻译领域面目全非，其实不只是机器翻译，是整个自然语言处理领域。博主目测现在自然语言处理只剩下两种任务<br/>
<li>被深度学习攻克了的</li>
<li>即将被深度学习攻克的</li>
笔者参加的学术活动不多，但每次参与学术活动，都会遇到一些学术界大小BOSS争论深度学习在自然语言处理中的前景。不知道二十几年前统计方法慢慢统治自然语言处理的时候，当时的学者是否也如此争论过。但是不得不说，统计方法走了十几、二十年的路才在自然语言处理任务中扎根并发扬光大，而深度学习如同暴风雨一般，一夜之间席卷一切。以统计机器翻译为例：走过了十年黄金期积累下来的各种复杂模型和理论，效果却不及提出仅两年的 NMT 端到端模型，难免让一些“恋旧”的学者伤心。统计机器翻译甚至还没有来得及打败更早实现的“基于人工规则的机器翻译”，就要走入寒冬了。博主的男神<a href="http://nlp.csai.tsinghua.edu.cn/~ly/" target="_blank">刘洋</a>老师是国内统计机器翻译领域的领军人物，现在也积极投入 NMT 的研究中，让博主十分欣慰。哦对了，面目全非的意思就是，一时间，在顶级会议论文集里面找不到 SMT 的论文了。。。
<br />
<h3>Bengio 团队</h3>
Bengio 是深度学习应用在自然语言处理任务的带头人之一，NMT的论文，Bengio 团队是主要的输出。不过斯坦福的 Manning 团队以其深厚的自然语言处理功底，应该会赶超的，加油。
<br />
<br />
<br />
<br />
<br />

<br /><font color="green">/******************************************  参考文线  *******************************************/</font><br />

1.<a id="bengio2003"/>Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A Neural Probabilistic Language Model. In JOURNAL OF MACHINE LEARNING RESEARCH.

2.<a id="mikolov2010"/>Mikolov, T., Karafiát, M., Burget, L., Cernocký, J., & Khudanpur, S. (2010, September). Recurrent neural network based language model. In INTERSPEECH (Vol. 2, p. 3).

3.<a id="mikolov2013"/>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).

4.<a id="duh2014"/>Duh, K. (2014). Deep Learning for Natural Language Processing and Machine Translation.

5.<a id="och2002"/>Och, F. J., & Ney, H. (2002, July). Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (pp. 295-302). Association for Computational Linguistics.

6.<a id="koehn2007"/>Koehn, P. (2007) et al., “Moses: open source toolkit for statistical machine translation. Proceedings of Acl,, 177--180.

7.<a id="schwenk2012"/>Schwenk, H. 2012. Continuous Space Translation Models for Phrase-Based Statistical Machine Translation. In COLING 2012: Posters, 1071-1080.

8.<a id="devlin2014"/>Devlin, J., R. Zbib, Z. Huang, T. Lamar, R. M. Schwartz & J. Makhoul. 2014. Fast and Robust Neural Network Joint Models for Statistical Machine Translation. In ACL (1), 1370-1380.: Citeseer.

9.<a id="och2003"/>Och, F. J. 2003. Minimum error rate training in statistical machine translation. In Meeting on Association for Computational Linguistics, 701-711.

10.<a id="sutskever2014"/>Sutskever, I., O. Vinyals & Q. V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, 3104-3112.

11.<a id="cho2014"/>Cho, K., B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk & Y. Bengio (2014) Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.

12.<a id="graves1997"/>Graves, A. (1997). Long short-term memory. Neural Computation, 9(8), 1735-80.

13.<a id="bahdanau2014"/>Bahdanau, D., K. Cho & Y. Bengio (2014) Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.

14.<a id="gulcehre2015"/>Gulcehre, C., O. Firat, K. Xu, K. Cho, L. Barrault, H. Lin, F. Bougares, H. Schwenk & Y. Bengio (2015) On using monolingual corpora in neural machine translation. arXiv preprint arXiv:1503.03535,.

15.<a id="firat2016"/>Firat, O., K. Cho & Y. Bengio (2016) Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism. arXiv preprint arXiv:1601.01073,.

16.<a id="jean2015"/>Jean S, Cho K, Memisevic R, Bengio Y. 2015. On Using Very Large Target Vocabulary forNeural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1–10, Beijing, China, July 26-31, 2015

17.<a id="luong2015a"/>Luong MT, Sutskever I, V.Le Q, Vinyals O, Zaremba W. 2015. Addressing the Rare Word Problem in Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 11–19, Beijing, China, July 26-31, 2015

18.<a id="chung2016"/>Chung, J., K. Cho & Y. Bengio (2016) A Character-level Decoder without Explicit Segmentation for Neural Machine Translation. arXiv preprint arXiv:1603.06147,.

19.<a id="vaswani2013"/>A Vaswani, Y Zhao, V Fossum, and D Chiang. 2013. Decoding with largescale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392, Seattle, Washington, USA, October. Association for Computational Linguistics

20.<a id="luong2015b"/>Luong, M., H. Pham, and C. D. Manning, 2015, Effective approaches to attention-based neural machine translation: arXiv preprint arXiv:1508.04025.

21.<a id="cheng2015"/>Cheng, Y., S. Shen, Z. He, W. He, H. Wu, M. Sun, and Y. Liu, 2015, Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation: arXiv preprint arXiv:1512.04650.

22.<a id="feng2016"/>Feng, S., S. Liu, M. Li, and M. Zhou, 2016, Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model: arXiv preprint arXiv:1601.03317.

23.<a id="tu2016"/>Tu, Z., Lu, Z., Liu, Y., Liu, X., & Li, H. (2016). Modeling Coverage for Neural Machine Translation. arXiv:1601.04811

24.<a id="gao2014"/>Gao, J., X. He, W. Yih, and L. Deng, 2014, Learning continuous phrase representations for translation modeling: In ACL

25.<a id="shen2015"/>Shen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. (2015). Minimum Risk Training for Neural Machine Translation. arXiv preprint arXiv:1512.02433.
