---
layout: post
type: "tech"
title:  "2016年度神经机器翻译前沿进展"
location: 东北老家
tag: "神经网络机器翻译 NMT"
categories: "NMT"
date:   2017-02-09 18:45:00 +0800
---


<a href="#kalchbrenner2013">Kalchbrenner and Blunsom, 2013</a> 最初引入神经机器翻译的思想，但翻译效果并不理想。之后，<a href="#cho2014">Cho et al., 2014</a>，<a href="#sutskever2014">Sutskever et al., 2014</a> 利用编码器-解码器框架实现神经机器翻译，又由 <a href="#bahdanau2014">Bahdanau et al., 2014</a> 为编码器-解码器引入注意力机制，这时的翻译效果已经追平并在某些方面超过统计机器翻译，也使得结合注意力机制的编码器-解码器框架成为了日后神经机器翻译的主要参考。

截至**2016年底**，神经机器翻译技术已经应用在 **Google**、**百度** 等公司的在线翻译系统中。下面将从**训练目标**、**外部记忆单元**、**词切片和字符级别翻译**、**多语言低资源翻译**四个方面对2016年神经机器翻译的主要进展进行简要介绍。

<h3>训练目标</h3>
神经机器翻译模型的训练目标对模型在翻译任务中的表现起着至关重要的作用。目前，使用较多的目标函数是极大似然估计，这种目标函数是定义在词级别上的，不包含上下文的信息，这对句子或篇章级别的机器翻译任务来说并不十分适合。为解决此类问题，可以将机器翻译任务的评价指标融入训练过程，提升模型训练与测试评价标准的一致性（<a href="#shen2016">Shen et al., 2016</a>; <a href="#ranzato2016">Ranzato et al., 2016</a>）；另一方面，神经机器翻译经常出现解码时丢失源语言信息的情况，针对此问题，可以将“还原源语言”作为训练目标融入训练过程（<a href="#cheng2016">Cheng et al. 2016</a>; <a href="#tu2016">Tu et al., 2016</a>），减少翻译时的源语言信息丢失（如图12）。

<figure align="center">
<img  src="{{ site.baseurl }}/images/2016nmtadvantage/xunlianmubiao2016.jpg" align="middle"  width="635" height="336"/>
<figcaption><b>图 1 将“还原源语言信息”引入训练目标。b为两种引入方法。</b></figcaption>
</figure>
<br />

<h3>外部记忆单元</h3>
RNN结构的网络采用固定长度的向量表示历史信息，对于不同长度的序列，长度越长则越容易损失更多的信息。借鉴神经图灵机（Neural Turing Machine，<a href="#graves2014">Graves et al., 2014</a>）的方法，为神经机器翻译中的RNN网络结构提供额外的记忆单元（如图13），有效缓解了RNN长距离信息衰减问题（<a href="#wang2016">Wang et al., 2016</a>; <a href="#meng2016">Meng et al., 2016</a>）。
<figure align="center">
<img  src="{{ site.baseurl }}/images/2016nmtadvantage/waibujiyidanyuan.jpg" align="middle"  width="249px" height="170px" />  
<figcaption><b>图 2 外部记忆单元</b></figcaption>
</figure>
<br />

<h3>词切片和字符级别翻译</h3>
神经机器翻译采用固定规模的词汇表面临着几个问题（1）解码时庞大的搜索空间影响效率；（2）“unk”和未登录词问题；（3）无法有效利用词形或组词的规则提供的信息，包括同根词或变形（如英语中的时态、单复数的变化）和新词，例如：
<figure align="center">
<img  src="{{ site.baseurl }}/images/2016nmtadvantage/ciqiepianhezifujibiefanyi.jpg" align="middle"  width="498" height="140"/>
</figure>
<br />

上述英文单词中，从单词本身的词形很容易推断出词性或语义信息。
词切片或字符级别的翻译不再使用单词作为构成词汇表的元素，而是采用比单词更细粒度的表示单元，以字符或词切片为例：
<figure align="center">
<img  src="{{ site.baseurl }}/images/2016nmtadvantage/ciqiepianhezifujibiefanyi2.jpg" align="middle"  width="249" height="135"/>
</figure>
<br />

这样可以减少词汇表规模、降低未登录词和“unk”的数目，并且将字形相近的词也关联起来。如果完全采用字符替代单词，则可以完全消除“unk”问题，且词汇表规模可以缩小到10^2~10^3。
在实践上，<a href="#luong2016">Luong et al., 2016</a> 提出了词语-字符混合的神经翻译模型；<a href="#sennrich2016">Sennrich et al., 2016</a> 提出利用字节对编码 （Byte Pair Encoding）建立基于子词（subword）的神经机器翻译模型；<a href="#chung2016">Chung et al., 2016</a> 提出基于字符的解码器；<a href="#lee2016">Lee et al., 2016</a> 提出一种完全基于字符的神经机器翻译模型，在编码端和解码端均使用字符作为输入、输出，并且尝试了基于字符的多语言翻译；Google的神经机器翻译系统（<a href="#wu2016">Wu et al., 2016</a>）采用词切片（wordpiece）作为神经机器翻译的输入和输出，在不降低翻译质量的情况下有效缩小词表长度。
上述方法采用比单词更细粒度的表示作为词汇表中的特征，负面的影响是增加了输入输出序列的长度：
<figure align="center">
<img  src="{{ site.baseurl }}/images/2016nmtadvantage/ciqiepianhezifujibiefanyi3.jpg" align="middle"  width="691" height="77"/>
</figure>
<br />

序列长度的增加对神经机器翻译的效果会有不利的影响，这是字符级别翻译面临的主要问题。

<h3>多语言低资源翻译</h3>
神经机器翻译中，解码器依据源语言的抽象表示进行解码，无论是编码器直接编码得到的上下文向量还是结合注意力机制之后的上下文向量均属于这种抽象表示，这种抽象表示为多语言低资源神经机器翻译提供了可能。传统的机器翻译方法，每个翻译系统只能实现在某一种语言对上特定方向的翻译，如果引入其他语言或者翻译方向，就要重新构建新的翻译系统；而多语言神经翻译则尝试实现同一模型完成多语言的翻译任务（如图14）。
<figure align="center">
<img  src="{{ site.baseurl }}/images/2016nmtadvantage/duoyuyandiziyuanfanyi.jpg" align="middle"  width="806" height="285"/>
<figcaption><b>图 3（a）传统多语言对翻译；（b）多语言神经机器翻译。</b></figcaption>
</figure>
<br />

另一方面，若图14中的中文和日文平行语料库稀缺，则无法利用统计机器翻译构建出中文到日文的直译系统，要实现中文到日文的机器翻译则需要以英文为枢轴语言（pivot language）完成：
<figure align="center">
<img  src="{{ site.baseurl }}/images/2016nmtadvantage/duoyuyandiziyuanfanyi2.jpg" align="middle"  width="795" height="90"/>
<figcaption><b>图 4 以英文为枢轴语言实现中文到日文的翻译。</b></figcaption>
</figure>
<br />

这种方法的问题在于：系统1产生的误差会传递到系统2并被放大，其准确率远低于利用大规模单一语言对训练的模型。而多语言神经机器翻译不但摆脱了枢轴语言，还可以利用不同语言之间的抽象表示的关联提升资源稀缺语言的翻译效果。  
多语言神经机器翻译的工作最初由 <a href="#firat2016a">Firat et al., 2016a</a> 提出，该工作实现了不同语言的编码器和解码器之间共享同一个注意力模型的方法，并对稀缺资源的翻译提供了帮助；类似地，<a href="#zoph2016">Zoph et al., 2016</a> 将在资源丰富语言平行语料库训练的模型参数迁移到资源匮乏语言翻译模型的训练过程中，帮助提升稀缺资源神经机器翻译效果；<a href="#firat2016b">Firat et al., 2016b</a> 对 <a href="#firat2016a">Firat et al., 2016a</a> 进行了改进，在源语言端将多语言的编码器融合，使其实现单一模型的多语到单语的多方向翻译 **Google** 公司的 <a href="#johson2016">Johson et al., 2016</a> 实现了严格意义上的多语言神经机器翻译系统。该工作实现了多语言共享同一词汇表的、多语言到多语言的神经机器翻译，且在0资源的翻译上取得可观效果，该工作还指出：多语言共享的注意力模型的输出可以看作抽象语义层面的“中间语言”，作为该多语言翻译神经翻译系统的理论解释。

<figure align="center">
<img  src="{{ site.baseurl }}/images/2016nmtadvantage/duoyuyandiziyuanfanyi3.jpg" align="middle"  width="722" height="156"/>
<figcaption><b>图 5 谷歌多语言翻译系统，通过在源语言端标记目标语言种类区分目标语言，如“<2en>”为翻译到英文；“<2zh>”为翻译到中文。</b></figcaption>
</figure>






<br /><font color="green">/******************************************  参考文线  *******************************************/</font><br />

1.<a id="kalchbrenner2013" /> Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent Continuous Translation Models. In Proceedings of EMNLP 2013  
2.<a id="sutskever2014" /> Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In Proceedings of NIPS.  
3.<a id="cho2014" /> Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio. 2014. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches. arXiv:1409.1259  
4.<a id="bahdanau2014" /> Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of ICLR.  
5.<a id="shen2016" /> Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Minimum Risk Training for Neural Machine Translation. In Proceedings of ACL.  
6.<a id="ranzato2016" /> Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence Level Training with Recurrent Neural Networks. In Proceedings of ICLR.  
7.<a id="cheng2016" /> Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016(b). Semi-Supervised Learning for Neural Machine Translation. In Proceedings of ACL.  
8.<a id="tu2016" /> Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu, Hang Li. 2016. Neural Machine Translation with Reconstruction. arXiv:1611.01874v2  
9.<a id="graves2014" /> Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural Turing Machines. arXiv:1410.5401v2.  
10.<a id="wang2016" /> Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun Liu. 2016. Memory-enhanced Decoder for Neural Machine Translation. In Proceedings of EMNLP.  
11.<a id="meng2016" /> Fandong Meng, Zhengdong Lu, Hang Li, Qun Liu. 2016. Interactive Attention for Neural Machine Translation. arXiv:1610.05011.  
12.<a id="luong2016" /> Minh-Thang Luong and Christopher Manning. 2016. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. In Proceedings of ACL.  
13.<a id="sennrich2016" /> Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of ACL.  
14.<a id="chung2016" /> Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. 2016. A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation. In Proceedings of ACL.  
15.<a id="lee2016" /> Jason Lee, Kyunghyun Cho, Thomas Hofmann. 2016. Fully Character-Level Neural Machine Translation without Explicit Segmentation . arXiv:1610.03017v2.  
16.<a id="wu2016" /> Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean. 2016. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv:1609.08144v2.  
17.<a id="firat2016a" /> Orhan Firat, Kyunghyun Cho, Yoshua Bengio. 2016(a). Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism. arXiv:1601.01073.   
18.<a id="zoph2016" /> Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer Learning for Low-Resource Neural Machine Translation. In Proceedings of EMNLP.  
19.<a id="firat2016b" /> Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan, Fatos T. Yarman Vural, Kyunghyun Cho. 2016(b). Zero-Resource Translation with Multi-Lingual Neural Machine Translation. In Proceedings of EMNLP.  
20.<a id="johson2016" /> Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean. 2016. Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. arXiv:1611.04558.  
